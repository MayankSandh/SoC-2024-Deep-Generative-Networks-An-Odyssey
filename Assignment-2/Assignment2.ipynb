{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\nclass VanillaVAE(tf.keras.Model):\n    def __init__(self, input_dim, output_dim, latent_dim=32):\n        super(VanillaVAE, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.latent_dim = latent_dim\n        \n        # Encoder\n        self.encoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=input_dim),\n            tf.keras.layers.Conv2D(32, 3, strides=2, activation='relu'),\n            tf.keras.layers.Conv2D(64, 3, strides=2, activation='relu'),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(latent_dim * 2)\n        ])\n        \n        # Decoder\n        self.decoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n            tf.keras.layers.Dense(input_dim[0]//4 * input_dim[1]//4 * 64, activation='relu'),\n            tf.keras.layers.Reshape((input_dim[0]//4, input_dim[1]//4, 64)),\n            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(output_dim[-1], 3, activation='sigmoid', padding='same')\n        ])\n    \n    def encode(self, x):\n        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n        return mean, logvar\n    \n    def reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def call(self, inputs):\n        mean, logvar = self.encode(inputs)\n        z = self.reparameterize(mean, logvar)\n        return self.decode(z), mean, logvar","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T18:55:28.213568Z","iopub.execute_input":"2024-07-12T18:55:28.214286Z","iopub.status.idle":"2024-07-12T18:55:42.715161Z","shell.execute_reply.started":"2024-07-12T18:55:28.214238Z","shell.execute_reply":"2024-07-12T18:55:42.714348Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-12 18:55:30.491661: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 18:55:30.491838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 18:55:30.661428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\nclass VAEEncoderDecoder(tf.keras.Model):\n    def __init__(self, input_dim, output_dim, latent_dim=32):\n        super(VAEEncoderDecoder, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.latent_dim = latent_dim\n        \n        # Encoder with encoder-decoder architecture\n        self.encoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=input_dim),\n            # Downsampling\n            tf.keras.layers.Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(64, 3, strides=2, activation='relu', padding='same'),\n            # Upsampling\n            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(latent_dim * 2)\n        ])\n        \n        # Decoder (same as in VanillaVAE)\n        self.decoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n            tf.keras.layers.Dense(input_dim[0]//4 * input_dim[1]//4 * 64, activation='relu'),\n            tf.keras.layers.Reshape((input_dim[0]//4, input_dim[1]//4, 64)),\n            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(output_dim[-1], 3, activation='sigmoid', padding='same')\n        ])\n    \n    def encode(self, x):\n        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n        return mean, logvar\n    \n    def reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def call(self, inputs):\n        mean, logvar = self.encode(inputs)\n        z = self.reparameterize(mean, logvar)\n        return self.decode(z), mean, logvar","metadata":{"execution":{"iopub.status.busy":"2024-07-12T18:58:21.479923Z","iopub.execute_input":"2024-07-12T18:58:21.481226Z","iopub.status.idle":"2024-07-12T18:58:21.497110Z","shell.execute_reply.started":"2024-07-12T18:58:21.481187Z","shell.execute_reply":"2024-07-12T18:58:21.496176Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nclass VAEDecoderEncoderDecoder(tf.keras.Model):\n    def __init__(self, input_dim, output_dim, latent_dim=32):\n        super(VAEDecoderEncoderDecoder, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.latent_dim = latent_dim\n        \n        # Encoder (same as in VanillaVAE)\n        self.encoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=input_dim),\n            tf.keras.layers.Conv2D(32, 3, strides=2, activation='relu'),\n            tf.keras.layers.Conv2D(64, 3, strides=2, activation='relu'),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(latent_dim * 2)\n        ])\n        \n        # Decoder with encoder-decoder architecture\n        self.decoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n            tf.keras.layers.Dense(input_dim[0]//4 * input_dim[1]//4 * 64, activation='relu'),\n            tf.keras.layers.Reshape((input_dim[0]//4, input_dim[1]//4, 64)),\n            # Upsampling\n            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            # Downsampling\n            tf.keras.layers.Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(64, 3, strides=2, activation='relu', padding='same'),\n            # Final upsampling\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(16, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(output_dim[-1], 3, activation='sigmoid', padding='same')\n        ])\n    \n    def encode(self, x):\n        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n        return mean, logvar\n    \n    def reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def call(self, inputs):\n        mean, logvar = self.encode(inputs)\n        z = self.reparameterize(mean, logvar)\n        return self.decode(z), mean, logvar","metadata":{"execution":{"iopub.status.busy":"2024-07-12T18:58:26.386231Z","iopub.execute_input":"2024-07-12T18:58:26.386886Z","iopub.status.idle":"2024-07-12T18:58:26.402731Z","shell.execute_reply.started":"2024-07-12T18:58:26.386854Z","shell.execute_reply":"2024-07-12T18:58:26.401791Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nclass VAEDoubleEncoderDecoder(tf.keras.Model):\n    def __init__(self, input_dim, output_dim, latent_dim=32):\n        super(VAEDoubleEncoderDecoder, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.latent_dim = latent_dim\n        \n        # Encoder with encoder-decoder architecture\n        self.encoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=input_dim),\n            # Downsampling\n            tf.keras.layers.Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(64, 3, strides=2, activation='relu', padding='same'),\n            # Upsampling\n            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(latent_dim * 2)\n        ])\n        \n        # Decoder with encoder-decoder architecture\n        self.decoder = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n            tf.keras.layers.Dense(input_dim[0]//4 * input_dim[1]//4 * 64, activation='relu'),\n            tf.keras.layers.Reshape((input_dim[0]//4, input_dim[1]//4, 64)),\n            # Upsampling\n            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            # Downsampling\n            tf.keras.layers.Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(64, 3, strides=2, activation='relu', padding='same'),\n            # Final upsampling\n            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2DTranspose(16, 3, strides=2, activation='relu', padding='same'),\n            tf.keras.layers.Conv2D(output_dim[-1], 3, activation='sigmoid', padding='same')\n        ])\n    \n    def encode(self, x):\n        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n        return mean, logvar\n    \n    def reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def call(self, inputs):\n        mean, logvar = self.encode(inputs)\n        z = self.reparameterize(mean, logvar)\n        return self.decode(z), mean, logvar","metadata":{"execution":{"iopub.status.busy":"2024-07-12T18:58:28.347617Z","iopub.execute_input":"2024-07-12T18:58:28.348261Z","iopub.status.idle":"2024-07-12T18:58:28.364902Z","shell.execute_reply.started":"2024-07-12T18:58:28.348229Z","shell.execute_reply":"2024-07-12T18:58:28.364003Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\n\n# Load and preprocess MNIST data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.expand_dims(x_train, axis=-1)\nx_test = np.expand_dims(x_test, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T18:58:29.667819Z","iopub.execute_input":"2024-07-12T18:58:29.668165Z","iopub.status.idle":"2024-07-12T18:58:30.178960Z","shell.execute_reply.started":"2024-07-12T18:58:29.668139Z","shell.execute_reply":"2024-07-12T18:58:30.177747Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndef train_and_evaluate_vae(model, epochs=30, batch_size=128):\n    optimizer = tf.keras.optimizers.Adam(1e-4)\n    \n    @tf.function\n    def train_step(x):\n        with tf.GradientTape() as tape:\n            reconstruction, mean, logvar = model(x)\n            reconstruction_loss = tf.reduce_mean(\n                tf.keras.losses.binary_crossentropy(x, reconstruction)\n            )\n            kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n            total_loss = reconstruction_loss + kl_loss\n        \n        grads = tape.gradient(total_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        return total_loss, reconstruction_loss, kl_loss\n    \n    test_losses = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        epoch_reconstruction_loss = 0\n        epoch_kl_loss = 0\n        num_batches = 0\n        \n        for batch in tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size):\n            total_loss, reconstruction_loss, kl_loss = train_step(batch)\n            epoch_loss += total_loss\n            epoch_reconstruction_loss += reconstruction_loss\n            epoch_kl_loss += kl_loss\n            num_batches += 1\n        \n        epoch_loss /= num_batches\n        epoch_reconstruction_loss /= num_batches\n        epoch_kl_loss /= num_batches\n        \n        # Evaluate the model\n        test_reconstruction, _, _ = model(x_test)\n        test_loss = tf.reduce_mean(\n            tf.keras.losses.binary_crossentropy(x_test, test_reconstruction)\n        )\n        test_losses.append(test_loss.numpy())\n        \n        print(f'Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}, '\n              f'Reconstruction Loss: {epoch_reconstruction_loss:.4f}, '\n              f'KL Loss: {epoch_kl_loss:.4f}, Test Loss: {test_loss:.4f}')\n    \n    # Generate and plot reconstructions for 3 sample inputs\n    sample_images = x_test[:3]\n    sample_reconstructions, _, _ = model(sample_images)\n    \n    plt.figure(figsize=(12, 4))\n    for i in range(3):\n        # Original\n        plt.subplot(2, 3, i+1)\n        plt.imshow(sample_images[i, :, :, 0], cmap='gray')\n        plt.title(f'Original {i+1}')\n        plt.axis('off')\n        \n        # Reconstruction\n        plt.subplot(2, 3, i+4)\n        plt.imshow(sample_reconstructions[i, :, :, 0], cmap='gray')\n        plt.title(f'Reconstructed {i+1}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'{model.__class__.__name__}_reconstructions.png')\n    plt.close()\n    \n    return test_losses[-1], test_losses","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:12:26.171678Z","iopub.execute_input":"2024-07-12T19:12:26.172070Z","iopub.status.idle":"2024-07-12T19:12:26.195184Z","shell.execute_reply.started":"2024-07-12T19:12:26.172042Z","shell.execute_reply":"2024-07-12T19:12:26.194137Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n# Train and evaluate all models\nmodels = {\n    'VanillaVAE': VanillaVAE(input_dim=(28, 28, 1), output_dim=(28, 28, 1)),\n    'VAEEncoderDecoder': VAEEncoderDecoder(input_dim=(28, 28, 1), output_dim=(28, 28, 1)),\n    'VAEDecoderEncoderDecoder': VAEDecoderEncoderDecoder(input_dim=(28, 28, 1), output_dim=(28, 28, 1)),\n    'VAEDoubleEncoderDecoder': VAEDoubleEncoderDecoder(input_dim=(28, 28, 1), output_dim=(28, 28, 1))\n}\n\nmodel_losses = {}","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:12:27.418921Z","iopub.execute_input":"2024-07-12T19:12:27.419641Z","iopub.status.idle":"2024-07-12T19:12:27.754583Z","shell.execute_reply.started":"2024-07-12T19:12:27.419602Z","shell.execute_reply":"2024-07-12T19:12:27.753736Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Function to plot test losses for all models\ndef plot_test_losses(model_losses):\n    plt.figure(figsize=(10, 6))\n    for model_name, losses in model_losses.items():\n        plt.plot(range(1, len(losses) + 1), losses, label=model_name)\n    plt.xlabel('Epoch')\n    plt.ylabel('Test Loss')\n    plt.title('Test Loss per Epoch for Different VAE Models')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig('vae_models_test_losses.png')\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:12:28.280973Z","iopub.execute_input":"2024-07-12T19:12:28.281324Z","iopub.status.idle":"2024-07-12T19:12:28.288045Z","shell.execute_reply.started":"2024-07-12T19:12:28.281298Z","shell.execute_reply":"2024-07-12T19:12:28.287086Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for model_name, model in models.items():\n    print(f\"\\nTraining {model_name}...\")\n    final_loss, epoch_losses = train_and_evaluate_vae(model)\n    model_losses[model_name] = epoch_losses\n    print(f'{model_name} Final Test Loss: {final_loss:.4f}')\n\n# Plot test losses for all models\nplot_test_losses(model_losses)\n\n# Find the best performing model\nbest_model = min(model_losses, key=lambda x: model_losses[x][-1])\nprint(f\"\\nBest performing model: {best_model}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:12:29.695667Z","iopub.execute_input":"2024-07-12T19:12:29.696687Z","iopub.status.idle":"2024-07-12T19:26:14.423569Z","shell.execute_reply.started":"2024-07-12T19:12:29.696655Z","shell.execute_reply":"2024-07-12T19:26:14.422648Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\nTraining VanillaVAE...\nEpoch 1, Train Loss: 0.3927, Reconstruction Loss: 0.3899, KL Loss: 0.0028, Test Loss: 0.2742\nEpoch 2, Train Loss: 0.2706, Reconstruction Loss: 0.2688, KL Loss: 0.0018, Test Loss: 0.2661\nEpoch 3, Train Loss: 0.2674, Reconstruction Loss: 0.2656, KL Loss: 0.0018, Test Loss: 0.2648\nEpoch 4, Train Loss: 0.2661, Reconstruction Loss: 0.2643, KL Loss: 0.0018, Test Loss: 0.2636\nEpoch 5, Train Loss: 0.2653, Reconstruction Loss: 0.2634, KL Loss: 0.0018, Test Loss: 0.2626\nEpoch 6, Train Loss: 0.2647, Reconstruction Loss: 0.2628, KL Loss: 0.0019, Test Loss: 0.2622\nEpoch 7, Train Loss: 0.2643, Reconstruction Loss: 0.2623, KL Loss: 0.0019, Test Loss: 0.2617\nEpoch 8, Train Loss: 0.2641, Reconstruction Loss: 0.2621, KL Loss: 0.0020, Test Loss: 0.2616\nEpoch 9, Train Loss: 0.2639, Reconstruction Loss: 0.2618, KL Loss: 0.0021, Test Loss: 0.2615\nEpoch 10, Train Loss: 0.2638, Reconstruction Loss: 0.2617, KL Loss: 0.0021, Test Loss: 0.2612\nEpoch 11, Train Loss: 0.2636, Reconstruction Loss: 0.2615, KL Loss: 0.0021, Test Loss: 0.2610\nEpoch 12, Train Loss: 0.2636, Reconstruction Loss: 0.2615, KL Loss: 0.0021, Test Loss: 0.2612\nEpoch 13, Train Loss: 0.2635, Reconstruction Loss: 0.2613, KL Loss: 0.0022, Test Loss: 0.2610\nEpoch 14, Train Loss: 0.2636, Reconstruction Loss: 0.2614, KL Loss: 0.0021, Test Loss: 0.2610\nEpoch 15, Train Loss: 0.2634, Reconstruction Loss: 0.2613, KL Loss: 0.0021, Test Loss: 0.2608\nEpoch 16, Train Loss: 0.2634, Reconstruction Loss: 0.2613, KL Loss: 0.0021, Test Loss: 0.2610\nEpoch 17, Train Loss: 0.2633, Reconstruction Loss: 0.2612, KL Loss: 0.0021, Test Loss: 0.2607\nEpoch 18, Train Loss: 0.2632, Reconstruction Loss: 0.2610, KL Loss: 0.0022, Test Loss: 0.2605\nEpoch 19, Train Loss: 0.2632, Reconstruction Loss: 0.2610, KL Loss: 0.0022, Test Loss: 0.2605\nEpoch 20, Train Loss: 0.2632, Reconstruction Loss: 0.2610, KL Loss: 0.0023, Test Loss: 0.2604\nEpoch 21, Train Loss: 0.2632, Reconstruction Loss: 0.2610, KL Loss: 0.0022, Test Loss: 0.2604\nEpoch 22, Train Loss: 0.2632, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2602\nEpoch 23, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2606\nEpoch 24, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0023, Test Loss: 0.2602\nEpoch 25, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2603\nEpoch 26, Train Loss: 0.2630, Reconstruction Loss: 0.2607, KL Loss: 0.0023, Test Loss: 0.2603\nEpoch 27, Train Loss: 0.2630, Reconstruction Loss: 0.2607, KL Loss: 0.0023, Test Loss: 0.2604\nEpoch 28, Train Loss: 0.2630, Reconstruction Loss: 0.2606, KL Loss: 0.0024, Test Loss: 0.2601\nEpoch 29, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2601\nEpoch 30, Train Loss: 0.2630, Reconstruction Loss: 0.2607, KL Loss: 0.0023, Test Loss: 0.2602\nVanillaVAE Final Test Loss: 0.2602\n\nTraining VAEEncoderDecoder...\nEpoch 1, Train Loss: 0.3757, Reconstruction Loss: 0.3724, KL Loss: 0.0033, Test Loss: 0.2729\nEpoch 2, Train Loss: 0.2701, Reconstruction Loss: 0.2682, KL Loss: 0.0019, Test Loss: 0.2658\nEpoch 3, Train Loss: 0.2671, Reconstruction Loss: 0.2652, KL Loss: 0.0019, Test Loss: 0.2641\nEpoch 4, Train Loss: 0.2660, Reconstruction Loss: 0.2640, KL Loss: 0.0020, Test Loss: 0.2634\nEpoch 5, Train Loss: 0.2650, Reconstruction Loss: 0.2631, KL Loss: 0.0020, Test Loss: 0.2628\nEpoch 6, Train Loss: 0.2646, Reconstruction Loss: 0.2626, KL Loss: 0.0020, Test Loss: 0.2619\nEpoch 7, Train Loss: 0.2642, Reconstruction Loss: 0.2622, KL Loss: 0.0020, Test Loss: 0.2615\nEpoch 8, Train Loss: 0.2641, Reconstruction Loss: 0.2620, KL Loss: 0.0021, Test Loss: 0.2616\nEpoch 9, Train Loss: 0.2640, Reconstruction Loss: 0.2619, KL Loss: 0.0021, Test Loss: 0.2612\nEpoch 10, Train Loss: 0.2638, Reconstruction Loss: 0.2617, KL Loss: 0.0021, Test Loss: 0.2614\nEpoch 11, Train Loss: 0.2638, Reconstruction Loss: 0.2616, KL Loss: 0.0021, Test Loss: 0.2611\nEpoch 12, Train Loss: 0.2636, Reconstruction Loss: 0.2615, KL Loss: 0.0022, Test Loss: 0.2611\nEpoch 13, Train Loss: 0.2635, Reconstruction Loss: 0.2613, KL Loss: 0.0023, Test Loss: 0.2607\nEpoch 14, Train Loss: 0.2635, Reconstruction Loss: 0.2612, KL Loss: 0.0023, Test Loss: 0.2610\nEpoch 15, Train Loss: 0.2635, Reconstruction Loss: 0.2612, KL Loss: 0.0023, Test Loss: 0.2609\nEpoch 16, Train Loss: 0.2634, Reconstruction Loss: 0.2612, KL Loss: 0.0022, Test Loss: 0.2609\nEpoch 17, Train Loss: 0.2634, Reconstruction Loss: 0.2611, KL Loss: 0.0022, Test Loss: 0.2605\nEpoch 18, Train Loss: 0.2633, Reconstruction Loss: 0.2611, KL Loss: 0.0022, Test Loss: 0.2608\nEpoch 19, Train Loss: 0.2634, Reconstruction Loss: 0.2611, KL Loss: 0.0022, Test Loss: 0.2607\nEpoch 20, Train Loss: 0.2632, Reconstruction Loss: 0.2610, KL Loss: 0.0022, Test Loss: 0.2604\nEpoch 21, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\nEpoch 22, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2602\nEpoch 23, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2603\nEpoch 24, Train Loss: 0.2632, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\nEpoch 25, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\nEpoch 26, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2604\nEpoch 27, Train Loss: 0.2631, Reconstruction Loss: 0.2608, KL Loss: 0.0023, Test Loss: 0.2602\nEpoch 28, Train Loss: 0.2630, Reconstruction Loss: 0.2607, KL Loss: 0.0024, Test Loss: 0.2600\nEpoch 29, Train Loss: 0.2629, Reconstruction Loss: 0.2605, KL Loss: 0.0024, Test Loss: 0.2600\nEpoch 30, Train Loss: 0.2630, Reconstruction Loss: 0.2605, KL Loss: 0.0025, Test Loss: 0.2599\nVAEEncoderDecoder Final Test Loss: 0.2599\n\nTraining VAEDecoderEncoderDecoder...\nEpoch 1, Train Loss: 0.3966, Reconstruction Loss: 0.3957, KL Loss: 0.0010, Test Loss: 0.2668\nEpoch 2, Train Loss: 0.2657, Reconstruction Loss: 0.2647, KL Loss: 0.0011, Test Loss: 0.2647\nEpoch 3, Train Loss: 0.2647, Reconstruction Loss: 0.2635, KL Loss: 0.0012, Test Loss: 0.2640\nEpoch 4, Train Loss: 0.2643, Reconstruction Loss: 0.2630, KL Loss: 0.0013, Test Loss: 0.2636\nEpoch 5, Train Loss: 0.2641, Reconstruction Loss: 0.2627, KL Loss: 0.0014, Test Loss: 0.2635\nEpoch 6, Train Loss: 0.2639, Reconstruction Loss: 0.2625, KL Loss: 0.0014, Test Loss: 0.2632\nEpoch 7, Train Loss: 0.2638, Reconstruction Loss: 0.2622, KL Loss: 0.0015, Test Loss: 0.2629\nEpoch 8, Train Loss: 0.2637, Reconstruction Loss: 0.2621, KL Loss: 0.0016, Test Loss: 0.2627\nEpoch 9, Train Loss: 0.2636, Reconstruction Loss: 0.2620, KL Loss: 0.0016, Test Loss: 0.2626\nEpoch 10, Train Loss: 0.2636, Reconstruction Loss: 0.2619, KL Loss: 0.0017, Test Loss: 0.2627\nEpoch 11, Train Loss: 0.2634, Reconstruction Loss: 0.2616, KL Loss: 0.0018, Test Loss: 0.2620\nEpoch 12, Train Loss: 0.2633, Reconstruction Loss: 0.2614, KL Loss: 0.0019, Test Loss: 0.2621\nEpoch 13, Train Loss: 0.2635, Reconstruction Loss: 0.2616, KL Loss: 0.0019, Test Loss: 0.2620\nEpoch 14, Train Loss: 0.2634, Reconstruction Loss: 0.2615, KL Loss: 0.0019, Test Loss: 0.2620\nEpoch 15, Train Loss: 0.2633, Reconstruction Loss: 0.2615, KL Loss: 0.0019, Test Loss: 0.2620\nEpoch 16, Train Loss: 0.2634, Reconstruction Loss: 0.2614, KL Loss: 0.0019, Test Loss: 0.2618\nEpoch 17, Train Loss: 0.2634, Reconstruction Loss: 0.2615, KL Loss: 0.0019, Test Loss: 0.2620\nEpoch 18, Train Loss: 0.2632, Reconstruction Loss: 0.2613, KL Loss: 0.0019, Test Loss: 0.2619\nEpoch 19, Train Loss: 0.2633, Reconstruction Loss: 0.2614, KL Loss: 0.0020, Test Loss: 0.2614\nEpoch 20, Train Loss: 0.2633, Reconstruction Loss: 0.2614, KL Loss: 0.0019, Test Loss: 0.2618\nEpoch 21, Train Loss: 0.2631, Reconstruction Loss: 0.2611, KL Loss: 0.0020, Test Loss: 0.2613\nEpoch 22, Train Loss: 0.2632, Reconstruction Loss: 0.2612, KL Loss: 0.0020, Test Loss: 0.2616\nEpoch 23, Train Loss: 0.2632, Reconstruction Loss: 0.2612, KL Loss: 0.0020, Test Loss: 0.2612\nEpoch 24, Train Loss: 0.2631, Reconstruction Loss: 0.2610, KL Loss: 0.0020, Test Loss: 0.2614\nEpoch 25, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2611\nEpoch 26, Train Loss: 0.2632, Reconstruction Loss: 0.2610, KL Loss: 0.0021, Test Loss: 0.2613\nEpoch 27, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2612\nEpoch 28, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2611\nEpoch 29, Train Loss: 0.2631, Reconstruction Loss: 0.2609, KL Loss: 0.0022, Test Loss: 0.2610\nEpoch 30, Train Loss: 0.2630, Reconstruction Loss: 0.2608, KL Loss: 0.0022, Test Loss: 0.2609\nVAEDecoderEncoderDecoder Final Test Loss: 0.2609\n\nTraining VAEDoubleEncoderDecoder...\nEpoch 1, Train Loss: 0.3586, Reconstruction Loss: 0.3578, KL Loss: 0.0008, Test Loss: 0.2683\nEpoch 2, Train Loss: 0.2661, Reconstruction Loss: 0.2653, KL Loss: 0.0008, Test Loss: 0.2656\nEpoch 3, Train Loss: 0.2648, Reconstruction Loss: 0.2639, KL Loss: 0.0008, Test Loss: 0.2646\nEpoch 4, Train Loss: 0.2643, Reconstruction Loss: 0.2633, KL Loss: 0.0010, Test Loss: 0.2643\nEpoch 5, Train Loss: 0.2640, Reconstruction Loss: 0.2628, KL Loss: 0.0011, Test Loss: 0.2638\nEpoch 6, Train Loss: 0.2639, Reconstruction Loss: 0.2626, KL Loss: 0.0013, Test Loss: 0.2636\nEpoch 7, Train Loss: 0.2637, Reconstruction Loss: 0.2624, KL Loss: 0.0014, Test Loss: 0.2633\nEpoch 8, Train Loss: 0.2637, Reconstruction Loss: 0.2622, KL Loss: 0.0015, Test Loss: 0.2628\nEpoch 9, Train Loss: 0.2635, Reconstruction Loss: 0.2619, KL Loss: 0.0016, Test Loss: 0.2625\nEpoch 10, Train Loss: 0.2634, Reconstruction Loss: 0.2617, KL Loss: 0.0017, Test Loss: 0.2626\nEpoch 11, Train Loss: 0.2635, Reconstruction Loss: 0.2617, KL Loss: 0.0018, Test Loss: 0.2621\nEpoch 12, Train Loss: 0.2634, Reconstruction Loss: 0.2616, KL Loss: 0.0018, Test Loss: 0.2626\nEpoch 13, Train Loss: 0.2633, Reconstruction Loss: 0.2614, KL Loss: 0.0019, Test Loss: 0.2621\nEpoch 14, Train Loss: 0.2633, Reconstruction Loss: 0.2613, KL Loss: 0.0020, Test Loss: 0.2620\nEpoch 15, Train Loss: 0.2633, Reconstruction Loss: 0.2614, KL Loss: 0.0020, Test Loss: 0.2618\nEpoch 16, Train Loss: 0.2633, Reconstruction Loss: 0.2614, KL Loss: 0.0020, Test Loss: 0.2620\nEpoch 17, Train Loss: 0.2633, Reconstruction Loss: 0.2613, KL Loss: 0.0020, Test Loss: 0.2618\nEpoch 18, Train Loss: 0.2632, Reconstruction Loss: 0.2610, KL Loss: 0.0022, Test Loss: 0.2613\nEpoch 19, Train Loss: 0.2630, Reconstruction Loss: 0.2607, KL Loss: 0.0023, Test Loss: 0.2613\nEpoch 20, Train Loss: 0.2631, Reconstruction Loss: 0.2605, KL Loss: 0.0026, Test Loss: 0.2609\nEpoch 21, Train Loss: 0.2631, Reconstruction Loss: 0.2605, KL Loss: 0.0026, Test Loss: 0.2609\nEpoch 22, Train Loss: 0.2632, Reconstruction Loss: 0.2605, KL Loss: 0.0026, Test Loss: 0.2610\nEpoch 23, Train Loss: 0.2631, Reconstruction Loss: 0.2604, KL Loss: 0.0027, Test Loss: 0.2605\nEpoch 24, Train Loss: 0.2630, Reconstruction Loss: 0.2600, KL Loss: 0.0029, Test Loss: 0.2605\nEpoch 25, Train Loss: 0.2629, Reconstruction Loss: 0.2598, KL Loss: 0.0031, Test Loss: 0.2600\nEpoch 26, Train Loss: 0.2630, Reconstruction Loss: 0.2596, KL Loss: 0.0034, Test Loss: 0.2599\nEpoch 27, Train Loss: 0.2628, Reconstruction Loss: 0.2594, KL Loss: 0.0035, Test Loss: 0.2598\nEpoch 28, Train Loss: 0.2628, Reconstruction Loss: 0.2592, KL Loss: 0.0036, Test Loss: 0.2597\nEpoch 29, Train Loss: 0.2627, Reconstruction Loss: 0.2590, KL Loss: 0.0037, Test Loss: 0.2593\nEpoch 30, Train Loss: 0.2627, Reconstruction Loss: 0.2589, KL Loss: 0.0039, Test Loss: 0.2586\nVAEDoubleEncoderDecoder Final Test Loss: 0.2586\n\nBest performing model: VAEDoubleEncoderDecoder\n","output_type":"stream"}]}]}